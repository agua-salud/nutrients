---
title: "3. Filtering"
description: |
  Reproducible workflow for Arbitrary, PERFect, & PIME filtering of community data.
author:
#  - name: Jarrod J Scott
#    url: https://example.com/norajones
#    affiliation: Spacely Sprockets
#    affiliation_nrl: https://example.com/spacelysprokets
bibliography: assets/cite.bib
output:
  distill::distill_article:
    toc: true
    toc_depth: 3
---

<details markdown="1">
<summary><strong>Click here</strong> for setup information.</summary>

```{r setup, message=FALSE, results = 'hide'}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
set.seed(119)
#library(conflicted)
#pacman::p_depends(PERFect, local = TRUE)  
#pacman::p_depends_reverse(PERFect, local = TRUE)  
library(phyloseq); packageVersion("phyloseq")
library(Biostrings); packageVersion("Biostrings")
pacman::p_load(tidyverse, patchwork, 
               agricolae, labdsv, naniar, PERFect, pime, 
               ape, gdata, microbiome, seqRFLP, 
               reactable, downloadthis, captioner, jamba, 
               install = FALSE, update = FALSE)

options(scipen=999)
knitr::opts_current$get(c(
  "cache",
  "cache.path",
  "cache.rebuild",
  "dependson",
  "autodep"
))

```

</details>

```{r, echo = FALSE, eval=TRUE, results = 'hide'}
# Create the caption(s) with captioner
remove(list = ls())
caption_tab_ssu <- captioner(prefix = "(16S rRNA) Table", suffix = " |", style = "b")
caption_fig_ssu <- captioner(prefix = "(16S rRNA) Figure", suffix = " |", style = "b")

caption_tab_its <- captioner(prefix = "(ITS) Table", suffix = " |", style = "b")
caption_fig_its <- captioner(prefix = "(ITS) Figure", suffix = " |", style = "b")
# Create a function for referring to the tables in text
ref <- function(x) str_extract(x, "[^|]*") %>% 
  trimws(which = "right", whitespace = "[ ]")
objects
```
</details>

```{r, echo = FALSE, eval = TRUE}
source("assets/captions/captions_filtering.R")
```

In [Part A](#a.-arbitrary-filtering), we apply arbitrary filtering to the 16S rRNA and ITS data sets. In [Part B](#b.-perfect-filtering) we use PERFect [(PERmutation Filtering test for microbiome data)](https://github.com/katiasmirn/PERFect) [@smirnova2019perfect] to filter the data sets. And in [Part C](#c.-pime-filtering) of this workflow, we use PIME [(Prevalence Interval for Microbiome Evaluation)](https://github.com/microEcology/pime) [@roesch2020pime] to filter the FULL 16S rRNA and ITS data sets. 

# Filtering Results

```{r, echo=FALSE, eval=TRUE}
load("page_build/filtering_summaries.rdata")
```

We begin by summarizing the results of each filtering method on the 16S and ITS data sets. Below you can find the complete workflow for each filtering method.

## 16S rRNA

<small>`r caption_tab_ssu("ssu_all_filt_summary")`</small>

```{r, echo=FALSE}
ssu_filtering_results <- dplyr::left_join(ssu_sum_arbitrary, ssu_sum_perfect, 
                                            by = c("Metric", "Full data")) %>% 
                           dplyr::left_join(., ssu_sum_pime, 
                                            by = c("Metric", "Full data")) %>%
  dplyr::rename("Arbitrary" = 3, "PERFect" = 4, "PIME" = 5)
```

```{r, echo=FALSE, eval=TRUE}
seq_table <- ssu_filtering_results
seq_table %>%
  download_this(
    output_name = "ssu_all_filt_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    Metric = colDef(name = "Metric", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200)
    ), 
  searchable = FALSE, defaultPageSize = 10, showPagination = FALSE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = FALSE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

```{r, echo=FALSE}
tmp_read_count <- data.frame(sample_sums(ssu_ps_work)) %>%
  tibble::rownames_to_column("SamName")
tmp_asv_count <- estimate_richness(ssu_ps_work, split = TRUE, measures = "Observed") %>%
  tibble::rownames_to_column("SamName")
tmp_samp_data <- data.frame(sample_data(ssu_ps_work))
tmp_samp_data <- tmp_samp_data[order(tmp_samp_data$YEAR, tmp_samp_data$TREAT),]

tmp_join <- dplyr::left_join(tmp_samp_data, tmp_read_count, by = "SamName") %>%
  dplyr::left_join(., tmp_asv_count, by = "SamName") %>%
  dplyr::rename("Sample_ID" = 1) %>%
  dplyr::rename("total_reads" = 9) %>%
  dplyr::rename("total_asvs" = 10)
ssu_work_samp_summary <- tmp_join
rm(list = ls(pattern = "tmp_"))
```

```{r, echo=FALSE}
ssu_filtering_results <- dplyr::left_join(ssu_sum_arbitrary, ssu_sum_perfect, 
                                            by = c("Metric", "Full data")) %>% 
                           dplyr::left_join(., ssu_sum_pime, 
                                            by = c("Metric", "Full data")) %>%
  dplyr::rename("Arbitrary" = 3, "PERFect" = 4, "PIME" = 5)
```

```{r, echo=FALSE}
ssu_all_filt_samp_summary <- dplyr::left_join(ssu_work_samp_summary, ssu_filt_samp_summary, 
                             by = c("Sample_ID", "LOC", "SITE", "TRAN", "YEAR", "AGE", "TREAT")) %>% 
            dplyr::left_join(., ssu_perfect_samp_summary, 
                             by = c("Sample_ID", "LOC", "SITE", "TRAN", "YEAR", "AGE", "TREAT")) %>% 
            dplyr::left_join(., ssu_pime_samp_summary, 
                             by = c("Sample_ID", "LOC", "SITE", "TRAN", "YEAR", "AGE", "TREAT", "YR_TREAT")) %>% 
            dplyr::relocate(c("total_reads.x", "total_reads.y", 
                              "total_reads.x.x", "total_reads.y.y"), .after = "YR_TREAT")

ssu_all_filt_samp_summary[,c(3:4, 8)] <- NULL
```

<small>`r caption_tab_ssu("ssu_all_filt_samp_summary")`</small>
```{r, echo=FALSE, eval=TRUE}
seq_table <- ssu_all_filt_samp_summary
seq_table %>%
  download_this(
    output_name = "ssu_all_filt_samp_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-page", eval=TRUE}
reactable(seq_table,
    columns = list(
    Sample_ID = colDef(name = "Sample_ID", 
                       sticky = "left", 
                       style = list(borderRight = "5px solid #eee"),
                       headerStyle = list(borderRight = "5px solid #eee"), 
                       align = "left",
                       minWidth = 200),
    total_reads.x = colDef(name = "FULL", filterable = FALSE, 
                       style = list(borderLeft = "5px solid #eee"),
                       headerStyle = list(borderLeft = "5px solid #eee")),
    total_reads.y = colDef(name = "Arbitrary", filterable = FALSE),
    total_reads.x.x = colDef(name = "PERFect", filterable = FALSE),
    total_reads.y.y = colDef(name = "PIME", filterable = FALSE, 
                       style = list(borderRight = "5px solid #eee"),
                       headerStyle = list(borderRight = "5px solid #eee")),
    total_asvs.x = colDef(name = "FULL", filterable = FALSE),
    total_asvs.y = colDef(name = "Arbitrary", filterable = FALSE),
    total_asvs.x.x = colDef(name = "PERFect", filterable = FALSE),
    total_asvs.y.y = colDef(name = "PIME", filterable = FALSE)
  ),
  columnGroups = list(
    colGroup(name = "Sample Data", columns = c("LOC", "YEAR", "AGE", "TREAT"),
                       headerStyle = list(fontSize = "1.5em")),
    colGroup(name = "Total Reads", columns = c("total_reads.x", "total_reads.y", "total_reads.x.x", "total_reads.y.y"),
                       headerStyle = list(borderLeft = "5px solid #eee", fontSize = "1.5em", borderRight = "5px solid #eee")),
    colGroup(name = "Total ASVs", columns = c("total_asvs.x", "total_asvs.y", "total_asvs.x.x", "total_asvs.y.y"),
                       headerStyle = list(fontSize = "1.5em"))
  ),
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = TRUE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  searchable = TRUE, defaultPageSize = 5, showPagination = TRUE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

## ITS

<small>`r caption_tab_its("its_all_filt_summary")`</small>

```{r, echo=FALSE}
its_filtering_results <- dplyr::left_join(its_sum_arbitrary, its_sum_perfect, 
                                            by = c("Metric", "Full data")) %>% 
                           dplyr::left_join(., its_sum_pime, 
                                            by = c("Metric", "Full data")) %>%
  dplyr::rename("Arbitrary" = 3, "PERFect" = 4, "PIME" = 5)
```

```{r, echo=FALSE, eval=TRUE}
seq_table <- its_filtering_results
seq_table %>%
  download_this(
    output_name = "its_all_filt_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    Metric = colDef(name = "Metric", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200)
    ), 
  searchable = FALSE, defaultPageSize = 10, showPagination = FALSE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = FALSE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

```{r, echo=FALSE}
tmp_read_count <- data.frame(sample_sums(its_ps_work)) %>%
  tibble::rownames_to_column("SamName")
tmp_asv_count <- estimate_richness(its_ps_work, split = TRUE, measures = "Observed") %>%
  tibble::rownames_to_column("SamName")
tmp_samp_data <- data.frame(sample_data(its_ps_work))
tmp_samp_data <- tmp_samp_data[order(tmp_samp_data$YEAR, tmp_samp_data$TREAT),]

tmp_join <- dplyr::left_join(tmp_samp_data, tmp_read_count, by = "SamName") %>%
  dplyr::left_join(., tmp_asv_count, by = "SamName") %>%
  dplyr::rename("Sample_ID" = 1) %>%
  dplyr::rename("total_reads" = 9) %>%
  dplyr::rename("total_asvs" = 10)
its_work_samp_summary <- tmp_join
rm(list = ls(pattern = "tmp_"))
```

```{r, echo=FALSE}
its_filtering_results <- dplyr::left_join(its_sum_arbitrary, its_sum_perfect, 
                                            by = c("Metric", "Full data")) %>% 
                           dplyr::left_join(., its_sum_pime, 
                                            by = c("Metric", "Full data")) %>%
  dplyr::rename("Arbitrary" = 3, "PERFect" = 4, "PIME" = 5)
```

```{r, echo=FALSE}
its_all_filt_samp_summary <- dplyr::left_join(its_work_samp_summary, its_filt_samp_summary, 
                             by = c("Sample_ID", "LOC", "SITE", "TRAN", "YEAR", "AGE", "TREAT")) %>% 
            dplyr::left_join(., its_perfect_samp_summary, 
                             by = c("Sample_ID", "LOC", "SITE", "TRAN", "YEAR", "AGE", "TREAT")) %>% 
            dplyr::left_join(., its_pime_samp_summary, 
                             by = c("Sample_ID", "LOC", "SITE", "TRAN", "YEAR", "AGE", "TREAT", "YR_TREAT")) %>% 
            dplyr::relocate(c("total_reads.x", "total_reads.y", 
                              "total_reads.x.x", "total_reads.y.y"), .after = "YR_TREAT")

its_all_filt_samp_summary[,c(3:4, 8)] <- NULL
```

<small>`r caption_tab_its("its_all_filt_samp_summary")`</small>
```{r, echo=FALSE, eval=TRUE}
seq_table <- its_all_filt_samp_summary
seq_table %>%
  download_this(
    output_name = "its_all_filt_samp_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-page", eval=TRUE}
reactable(seq_table,
    columns = list(
    Sample_ID = colDef(name = "Sample_ID", 
                       sticky = "left", 
                       style = list(borderRight = "5px solid #eee"),
                       headerStyle = list(borderRight = "5px solid #eee"), 
                       align = "left",
                       minWidth = 200),
    total_reads.x = colDef(name = "FULL", filterable = FALSE, 
                       style = list(borderLeft = "5px solid #eee"),
                       headerStyle = list(borderLeft = "5px solid #eee")),
    total_reads.y = colDef(name = "Arbitrary", filterable = FALSE),
    total_reads.x.x = colDef(name = "PERFect", filterable = FALSE),
    total_reads.y.y = colDef(name = "PIME", filterable = FALSE, 
                       style = list(borderRight = "5px solid #eee"),
                       headerStyle = list(borderRight = "5px solid #eee")),
    total_asvs.x = colDef(name = "FULL", filterable = FALSE),
    total_asvs.y = colDef(name = "Arbitrary", filterable = FALSE),
    total_asvs.x.x = colDef(name = "PERFect", filterable = FALSE),
    total_asvs.y.y = colDef(name = "PIME", filterable = FALSE)
  ),
  columnGroups = list(
    colGroup(name = "Sample Data", columns = c("LOC", "YEAR", "AGE", "TREAT"),
                       headerStyle = list(fontSize = "1.5em")),
    colGroup(name = "Total Reads", columns = c("total_reads.x", "total_reads.y", "total_reads.x.x", "total_reads.y.y"),
                       headerStyle = list(borderLeft = "5px solid #eee", fontSize = "1.5em", borderRight = "5px solid #eee")),
    colGroup(name = "Total ASVs", columns = c("total_asvs.x", "total_asvs.y", "total_asvs.x.x", "total_asvs.y.y"),
                       headerStyle = list(fontSize = "1.5em"))
  ),
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = TRUE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  searchable = TRUE, defaultPageSize = 5, showPagination = TRUE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

```{r, echo=FALSE, eval=TRUE}
## Load to build page
gdata::keep(caption_fig_its, caption_fig_ssu, 
            caption_tab_its, caption_tab_ssu, 
            ref, sure = TRUE)
source("assets/captions/captions_filtering.R")
load("page_build/filtering_wf.rdata")
```

# A. Arbitrary Filtering

For low-count arbitrary filtering, we set the minimum read count to `5` and the prevalence to 20%. Then we apply another filter to remove ASVs with a low variance (`0.2`). 

```{r, echo=FALSE}
## Initial Load for ARBITRARY  ANALYSIS #1
set.seed(119)
ssu_ps_work <- readRDS("files/data-prep/rdata/ssu_ps_work.rds")
its_ps_work <- readRDS("files/data-prep/rdata/its_ps_work.rds")
```

## 16S rRNA

```{r, code_folding=TRUE}
tmp_low_count <- phyloseq::genefilter_sample(
                                         ssu_ps_work, 
                                         filterfun_sample(function(x) x >= 5), 
                                         A = 0.2 * nsamples(ssu_ps_work))
tmp_low_count <- phyloseq::prune_taxa(tmp_low_count, ssu_ps_work)
tmp_low_var <- phyloseq::filter_taxa(tmp_low_count, 
                                     function(x) var(x) > 0.2, prune = TRUE)
ssu_ps_filt <- tmp_low_var
ssu_ps_filt@phy_tree <- NULL
tmp_tree <- rtree(ntaxa(ssu_ps_filt), rooted = TRUE,
                      tip.label = taxa_names(ssu_ps_filt))
ssu_ps_filt <- merge_phyloseq(ssu_ps_filt,
                          sample_data,
                          tmp_tree)

rm(list = ls(pattern = "tmp_"))
```

Here is the filtered 16S rRNA phyloseq object. 

```{r, echo=FALSE, eval=TRUE}
ssu_ps_filt
```

```{r, echo=FALSE}
## This code is gross. I spent hours trying to find a better
## solution but failed :(
rm(list = ls(pattern = "tmp_"))
tmp_samples <- c("ssu_ps_work", "ssu_ps_filt")
tmp_objects <- c(
                "Total number of reads", 
                "Min. number of reads", 
                "Max. number of reads", 
                "Average number of reads", 
                "Median number of reads", 
                "Total ASVs", 
                "Min. number of ASVs", 
                "Max. number of ASVs", 
                "Average number of ASVs", 
                "Median number of ASVs")

tmp_total_reads <- c()
for (i in tmp_samples) {
   tmp_get <- sum(readcount(get(i)))
   tmp_total_reads <- c(append(tmp_total_reads, tmp_get))
}
tmp_min_read <- c()
for (i in tmp_samples) {
   tmp_get <- min(readcount(get(i)))
   tmp_min_read <- c(append(tmp_min_read, tmp_get))
}
tmp_max_read <- c()
for (i in tmp_samples) {
   tmp_get <- max(readcount(get(i)))
   tmp_max_read <- c(append(tmp_max_read, tmp_get))
}
tmp_mean_reads <- c()
for (i in tmp_samples) {
   tmp_get <- round(mean(readcount(get(i))), digits = 0)
   tmp_mean_reads <- c(append(tmp_mean_reads, tmp_get))
}
tmp_median_reads <- c()
for (i in tmp_samples) {
   tmp_get <- median(readcount(get(i)))
   tmp_median_reads <- c(append(tmp_median_reads, tmp_get))
}
tmp_total_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- ntaxa(get(i))
   tmp_total_asvs <- c(append(tmp_total_asvs, tmp_get))
}
tmp_min_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- min(richness(get(i), index = "observed"))
   tmp_min_asvs <- c(append(tmp_min_asvs, tmp_get))
}
tmp_max_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- max(richness(get(i), index = "observed"))
   tmp_max_asvs <- c(append(tmp_max_asvs, tmp_get))
}
tmp_mean_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- round(mean(richness(get(i), index = "observed")$observed), digits = 0)
   tmp_mean_asvs <- c(append(tmp_mean_asvs, tmp_get))
}
tmp_med_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- median(richness(get(i), index = "observed")$observed)
   tmp_med_asvs <- c(append(tmp_med_asvs, tmp_get))
}

tmp_bind <- data.frame(do.call("rbind", list(
  tmp_total_reads, tmp_min_read, tmp_max_read, tmp_mean_reads, tmp_median_reads, 
  tmp_total_asvs, tmp_min_asvs, tmp_max_asvs, tmp_mean_asvs, tmp_med_asvs)))

ssu_sum_arbitrary <- dplyr::bind_cols(tmp_objects, tmp_bind) %>%
  dplyr::rename("Metric" = 1, "Full data" = 2, "Arbitrary filter" = 3)
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_ssu("ssu_arbitrary_filt_summary")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- ssu_sum_arbitrary
seq_table %>%
  download_this(
    output_name = "ssu_arbitrary_filt_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 150
    ), 
  columns = list(
    Metric = colDef(name = "Metric", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200)
    ), 
  searchable = FALSE, defaultPageSize = 10, showPagination = FALSE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = FALSE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

We can look at how filtering affected total reads and ASVs for each sample.

```{r, echo=FALSE}
tmp_read_count <- data.frame(sample_sums(ssu_ps_filt)) %>%
  tibble::rownames_to_column("SamName")
tmp_asv_count <- estimate_richness(ssu_ps_filt, split = TRUE, measures = "Observed") %>%
  tibble::rownames_to_column("SamName")
tmp_samp_data <- data.frame(sample_data(ssu_ps_filt))
tmp_samp_data <- tmp_samp_data[order(tmp_samp_data$LOC, tmp_samp_data$YEAR),]

tmp_join <- dplyr::left_join(tmp_samp_data, tmp_read_count, by = "SamName") %>%
  dplyr::left_join(., tmp_asv_count, by = "SamName") %>%
  dplyr::rename("Sample_ID" = 1) %>%
  dplyr::rename("total_reads" = 8) %>%
  dplyr::rename("total_asvs" = 9)
ssu_filt_samp_summary <- tmp_join
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_ssu("ssu_filt_samp_summary")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- ssu_filt_samp_summary
seq_table %>%
  download_this(
    output_name = "ssu_filt_samp_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-page", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = TRUE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    Sample_ID = colDef(name = "Sample_ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200),
     total_reads = colDef(name = "total reads", filterable = FALSE, minWidth = 140),
     total_asvs = colDef(name = "total asvs", filterable = FALSE, minWidth = 140)
    ), 
  searchable = TRUE, defaultPageSize = 5, showPagination = TRUE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

## ITS

```{r, code_folding=TRUE}
tmp_low_count <- phyloseq::genefilter_sample(
                                         its_ps_work, 
                                         filterfun_sample(function(x) x >= 5), 
                                         A = 0.2 * nsamples(its_ps_work))
tmp_low_count <- phyloseq::prune_taxa(tmp_low_count, its_ps_work)
tmp_low_var <- phyloseq::filter_taxa(tmp_low_count, 
                                     function(x) var(x) > 0.2, prune = TRUE)
its_ps_filt <- tmp_low_var
rm(list = ls(pattern = "tmp_"))
its_ps_filt
```

And the filtered ITS phyloseq object. 

```{r, echo=FALSE, eval=TRUE}
its_ps_filt
```

```{r, echo=FALSE}
## This code is gross. I spent hours trying to find a better
## solution but failed :(
rm(list = ls(pattern = "tmp_"))
tmp_samples <- c("its_ps_work", "its_ps_filt")
tmp_objects <- c(
                "Total number of reads", 
                "Min. number of reads", 
                "Max. number of reads", 
                "Average number of reads", 
                "Median number of reads", 
                "Total ASVs", 
                "Min. number of ASVs", 
                "Max. number of ASVs", 
                "Average number of ASVs", 
                "Median number of ASVs")

tmp_total_reads <- c()
for (i in tmp_samples) {
   tmp_get <- sum(readcount(get(i)))
   tmp_total_reads <- c(append(tmp_total_reads, tmp_get))
}
tmp_min_read <- c()
for (i in tmp_samples) {
   tmp_get <- min(readcount(get(i)))
   tmp_min_read <- c(append(tmp_min_read, tmp_get))
}
tmp_max_read <- c()
for (i in tmp_samples) {
   tmp_get <- max(readcount(get(i)))
   tmp_max_read <- c(append(tmp_max_read, tmp_get))
}
tmp_mean_reads <- c()
for (i in tmp_samples) {
   tmp_get <- round(mean(readcount(get(i))), digits = 0)
   tmp_mean_reads <- c(append(tmp_mean_reads, tmp_get))
}
tmp_median_reads <- c()
for (i in tmp_samples) {
   tmp_get <- median(readcount(get(i)))
   tmp_median_reads <- c(append(tmp_median_reads, tmp_get))
}
tmp_total_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- ntaxa(get(i))
   tmp_total_asvs <- c(append(tmp_total_asvs, tmp_get))
}
tmp_min_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- min(richness(get(i), index = "observed"))
   tmp_min_asvs <- c(append(tmp_min_asvs, tmp_get))
}
tmp_max_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- max(richness(get(i), index = "observed"))
   tmp_max_asvs <- c(append(tmp_max_asvs, tmp_get))
}
tmp_mean_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- round(mean(richness(get(i), index = "observed")$observed), digits = 0)
   tmp_mean_asvs <- c(append(tmp_mean_asvs, tmp_get))
}
tmp_med_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- median(richness(get(i), index = "observed")$observed)
   tmp_med_asvs <- c(append(tmp_med_asvs, tmp_get))
}

tmp_bind <- data.frame(do.call("rbind", list(
  tmp_total_reads, tmp_min_read, tmp_max_read, tmp_mean_reads, tmp_median_reads, 
  tmp_total_asvs, tmp_min_asvs, tmp_max_asvs, tmp_mean_asvs, tmp_med_asvs)))

its_sum_arbitrary <- dplyr::bind_cols(tmp_objects, tmp_bind) %>%
  dplyr::rename("Metric" = 1, "Full data" = 2, "Arbitrary filter" = 3)
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_its("its_arbitrary_filt_summary")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- its_sum_arbitrary
seq_table %>%
  download_this(
    output_name = "its_arbitrary_filt_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 150
    ), 
  columns = list(
    Metric = colDef(name = "Metric", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200),
     total_reads = colDef(name = "total reads", filterable = FALSE, minWidth = 140),
     total_asvs = colDef(name = "total asvs", filterable = FALSE, minWidth = 140)
    ), 
  searchable = FALSE, defaultPageSize = 10, showPagination = FALSE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = FALSE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = FALSE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

And again, let's look how filtering affected total reads and ASVs for each sample.

```{r, echo=FALSE}
tmp_read_count <- data.frame(sample_sums(its_ps_filt)) %>%
  tibble::rownames_to_column("SamName")
tmp_asv_count <- estimate_richness(its_ps_filt, split = TRUE, measures = "Observed") %>%
  tibble::rownames_to_column("SamName")
tmp_samp_data <- data.frame(sample_data(its_ps_filt))
tmp_samp_data <- tmp_samp_data[order(tmp_samp_data$LOC, tmp_samp_data$YEAR),]

tmp_join <- dplyr::left_join(tmp_samp_data, tmp_read_count, by = "SamName") %>%
  dplyr::left_join(., tmp_asv_count, by = "SamName") %>%
  dplyr::rename("Sample_ID" = 1) %>%
  dplyr::rename("total_reads" = 8) %>%
  dplyr::rename("total_asvs" = 9)
its_filt_samp_summary <- tmp_join
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_its("its_filt_samp_summary")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- its_filt_samp_summary
seq_table %>%
  download_this(
    output_name = "its_filt_samp_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-page", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = TRUE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    Sample_ID = colDef(name = "Sample_ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200),
     total_reads = colDef(name = "total reads", filterable = FALSE, minWidth = 140),
     total_asvs = colDef(name = "total asvs", filterable = FALSE, minWidth = 140)
    ), 
  searchable = TRUE, defaultPageSize = 5, showPagination = TRUE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

```{r, echo=FALSE}
saveRDS(ssu_ps_filt, "files/filtering/arbitrary/rdata/ssu_ps_filt.rds")
saveRDS(its_ps_filt, "files/filtering/arbitrary/rdata/its_ps_filt.rds")
save.image("page_build/filtering_wf.rdata")
```

And that's it for Arbitrary filtering. Moving on.

# B. PERFect Filtering

```{r, echo=FALSE, eval=FALSE}
## Load to build page
## perfect_wf_alt.rdata = ITS pval  set at  0.05: 1712 ASVs
## perfect_wf.rdata = ITS pval  set at  0.10: 3299 ASVs
load("page_build/perfect_wf_alt.rdata")
objects()
ssu_ps_work <- readRDS("files/data-prep/rdata/ssu_ps_work.rds")
its_ps_work <- readRDS("files/data-prep/rdata/its_ps_work.rds")
```

To run PERFect, we need the ASV tables in data frame format with samples as rows and ASVs as columns. PERFect is sensitive to the order of ASVs, so here we test **a**) the default order in the phyloseq object and **b**) ASVs ordered by decreasing abundance.

## Setup

```{r, code_folding=TRUE}
samp_ps_filt <- c("ssu_ps_work", "its_ps_work")
for (i in samp_ps_filt) {
  tmp_get <- get(i)
  tmp_get_tab <- data.frame(t(otu_table(tmp_get)))
  tmp_get_tab <- tmp_get_tab %>% tibble::rownames_to_column("ID")
  tmp_get_tab <- jamba::mixedSortDF(tmp_get_tab, decreasing = FALSE, 
                                        useRownames = FALSE, byCols = 1)
  tmp_get_tab <- tmp_get_tab %>% tibble::remove_rownames() 
  tmp_get_tab <- tmp_get_tab %>% tibble::column_to_rownames("ID")
  tmp_get_tab <- data.frame(t(tmp_get_tab))

  tmp_get_tab_ord <- data.frame(t(otu_table(tmp_get)))
  tmp_get_tab_ord <- tmp_get_tab_ord %>% tibble::rownames_to_column("ID")
  tmp_get_tab_ord <- jamba::mixedSortDF(tmp_get_tab_ord, decreasing = TRUE, 
                                        useRownames = FALSE, byCols = 1)
  tmp_get_tab_ord <- tmp_get_tab_ord %>% tibble::remove_rownames() 
  tmp_get_tab_ord <- tmp_get_tab_ord %>% tibble::column_to_rownames("ID")
  tmp_get_tab_ord <- data.frame(t(tmp_get_tab_ord))

  tmp_tab_name <- purrr::map_chr(i, ~ paste0(., "_perfect"))
  assign(tmp_tab_name, tmp_get_tab)
  
  tmp_tab_ord_name <- purrr::map_chr(i, ~ paste0(., "_ord_perfect"))
  assign(tmp_tab_ord_name, tmp_get_tab_ord)
  
  rm(list = ls(pattern = "tmp_"))
}
objects()
```

## Filter 

Next we run the filtering analysis using [`PERFect_sim`](https://rdrr.io/github/katiasmirn/PERFect/man/PERFect_sim.html). The other option is [`PERFect_perm`](https://rdrr.io/github/katiasmirn/PERFect/man/PERFect_perm.html) however I could not get `PERFect_perm` to work as of this writing. The process never finished :/

We need to set an initial p-value cutoff. For 16S rRNA, we use `0.05` and for ITS we use `0.05`. 

```{r}
# Set a pvalue cutoff
ssu_per_pval <- 0.05
its_per_pval <- 0.05
```

```{r, code_folding=TRUE}
for (i in samp_ps_filt) {
  tmp_get <- get(purrr::map_chr(i, ~ paste0(., "_perfect")))
  tmp_pval <- gsub("_.*", "_per_pval", i)
  tmp_get_ord <- get(purrr::map_chr(i, ~ paste0(., "_ord_perfect")))
  tmp_sim <- PERFect_sim(X = tmp_get, alpha = get(tmp_pval), Order = "NP", center = FALSE)
  dim(tmp_sim$filtX)
  tmp_sim_ord <- PERFect_sim(X = tmp_get_ord, alpha = get(tmp_pval), Order = "NP", center = FALSE)
  dim(tmp_sim_ord$filtX)
  
  tmp_sim_name <- purrr::map_chr(i, ~ paste0(., "_perfect_sim"))
  assign(tmp_sim_name, tmp_sim)

  tmp_sim_ord_name <- purrr::map_chr(i, ~ paste0(., "_ord_perfect_sim"))
  assign(tmp_sim_ord_name, tmp_sim_ord)
  
  tmp_path <- file.path("files/filtering/perfect/rdata/")
  saveRDS(tmp_sim, paste(tmp_path, tmp_sim_name, ".rds", sep = ""))
  saveRDS(tmp_sim_ord, paste(tmp_path, tmp_sim_ord_name, ".rds", sep = ""))

  rm(list = ls(pattern = "tmp_"))
  
}
objects(pattern = "_sim")
```

```{r, echo=FALSE}
## FOR TESTING
dim(ssu_ps_work_perfect_sim$filtX)
dim(ssu_ps_work_ord_perfect_sim$filtX)
dim(its_ps_work_perfect_sim$filtX)
dim(its_ps_work_ord_perfect_sim$filtX)
###########
tmp_obj <- ssu_ps_work_ord_perfect_sim
tmp_ps <- ssu_ps_work
###########
tmp_filt <- data.frame(t(tmp_obj$filtX))
tmp_filt <- tmp_filt %>% tibble::rownames_to_column("ID")
tmp_pval <- data.frame(tmp_obj$pvals)
tmp_pval <- tmp_pval %>% tibble::rownames_to_column("ID")
tmp_org <- data.frame(t(otu_table(tmp_ps)))
tmp_org <- tmp_org %>% tibble::rownames_to_column("ID")
tmp_com <- dplyr::left_join(tmp_pval, tmp_org, by = "ID") %>%
           dplyr::left_join(., tmp_filt, by = "ID")
write.table(tmp_com, "tmp_com.txt", quote = FALSE, sep = "\t", row.names = FALSE)
```

```{r, echo=FALSE}
ps_sim <- c("ssu_ps_work_perfect_sim", "ssu_ps_work_ord_perfect_sim", 
            "its_ps_work_perfect_sim", "its_ps_work_ord_perfect_sim")
for (i in ps_sim) {
  tmp_get <- get(i)
  tmp_name <- i
  tmp_get$info <- NULL
  tmp_get$DFL <- NULL
  tmp_get$est <- NULL
  tmp_get$fit <- NULL
  tmp_get$hist <- NULL
  tmp_get$pDFL <- NULL
  assign(tmp_name, tmp_get)
  rm(list = ls(pattern = "tmp_"))
}  
objects()
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Save some deets so we do not need to save large objects
ssu_default_num_asvs <- dim(ssu_ps_work_perfect_sim$filtX)[2]
ssu_ord_num_asvs <- dim(ssu_ps_work_ord_perfect_sim$filtX)[2]
its_default_num_asvs <- dim(its_ps_work_perfect_sim$filtX)[2]
its_ord_num_asvs <- dim(its_ps_work_ord_perfect_sim$filtX)[2]

ssu_default_pvals <- ssu_ps_work_perfect_sim$pvals
ssu_ord_pvals <- ssu_ps_work_ord_perfect_sim$pvals
its_default_pvals <- its_ps_work_perfect_sim$pvals
its_ord_pvals <- its_ps_work_ord_perfect_sim$pvals
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
#Save some deets so we do not need to save large objects
ssu_default_num_asvs <- 58311
ssu_ord_num_asvs <- 19706
its_default_num_asvs <- 33657
its_ord_num_asvs <- 12934

ssu_default_pvals <- 4616
ssu_ord_pvals <- 4615
its_default_pvals <- 1712
its_ord_pvals <- 1765
```

How many ASVs were retained after filtering? 

First the 16S rRNA data set. Default ordering resulted in 
**`r ssu_default_num_asvs`** ASVs 
and reordering the data resulted in 
**`r ssu_ord_num_asvs`** ASVs.

And then the ITS data set. Default ordering resulted in 
**`r its_default_num_asvs`** ASVs 
and reordering the data resulted in 
**`r its_ord_num_asvs`** ASVs.

For some reason, the package does not remove based on the p value cutoff that we set earlier (`0.05`). So we need to filter out the ASVs that have a higher p-value than the cutoff.

```{r echo=FALSE, results='hold', eval=FALSE, render=pander::pander}
cat("Total 16S rRNA ASVs with p-value less than", ssu_per_pval[1], "\n")
tmp_df <- ssu_default_pvals
tmp_df <- data.frame(tmp_df)
pval_asv <- tmp_df %>% dplyr::summarise(count = sum(tmp_df <= ssu_per_pval))
print(paste("default order: ASVs before checking p value was", ssu_default_num_asvs, "and after was", pval_asv$count[1]), quote = FALSE)

tmp_df <- ssu_ord_pvals
tmp_df <- data.frame(tmp_df)
pval_asv_ord <- tmp_df %>% dplyr::summarise(count = sum(tmp_df <= ssu_per_pval))
print(paste("decreasing order: ASVs before checking p value was", ssu_ord_num_asvs, "and after was", pval_asv_ord$count[1]), quote = FALSE)

print("--------------------------------------", quote = FALSE)

cat("Total ITS ASVs with p-value less than", its_per_pval[1], "\n")
tmp_df <- its_default_pvals
tmp_df <- data.frame(tmp_df)
pval_asv <- tmp_df %>% dplyr::summarise(count = sum(tmp_df <= its_per_pval))
print(paste("default order: ASVs before checking p-value was", its_default_num_asvs, "and after was", pval_asv$count[1]), quote = FALSE)

tmp_df <- its_ord_pvals
tmp_df <- data.frame(tmp_df)
pval_asv_ord <- tmp_df %>% dplyr::summarise(count = sum(tmp_df <= its_per_pval))
print(paste("decreasing order: ASVs before checking p-value was", its_ord_num_asvs, "and after was", pval_asv_ord$count[1]), quote = FALSE)
```

```{r echo=FALSE, results='hold', eval=TRUE, render=pander::pander}
print("Total 16S rRNA ASVs with p-value less than 0.05")
print("[1] default order: ASVs before checking p value was 58311 and after was 4616")
print("[1] decreasing order: ASVs before checking p value was 19706 and after was 4615")
print("[1] --------------------------------------")
print("Total ITS ASVs with p-value less than 0.05 ")
print("[1] default order: ASVs before checking p-value was 33657 and after was 1712")
print("[1] decreasing order: ASVs before checking p-value was 12934 and after was 1765")
```

Now we can make phyloseq objects. Manual inspection of the results from `PERFect_sim` for the 16S rRNA data indicated that using the **decreasing** order and filtering p-values less than `r ssu_per_pval` yielded in the best results. Manual inspection of the results from `PERFect_sim` for the ITS data indicated that using the **default** order and filtering p-values less than `r its_per_pval` yielded in the best results. These approaches limited the number of ASVs found in only 1 or 2 samples. So first we filter out ASVs with p-values lower than the defined cutoff and then make the objects.

```{r}
# pvalue cutoffs set earlier
ssu_per_pval <- 0.05
its_per_pval <- 0.05
# Choose method
ssu_select <- "_ord_perfect"
its_select <- "_perfect"
```


```{r, code_folding=TRUE}
for (i in samp_ps_filt) {
  ## select pval cutoff and method
    tmp_pval <- gsub("18.*", "_per_pval", i)
    tmp_select <- gsub("18.*", "_select", i)
    
    tmp_get <- get(purrr::map_chr(i, ~ paste0(., get(tmp_select))))
    tmp_get_sim <- get(purrr::map_chr(i, ~ paste0(., get(tmp_select), "_sim")))
    tmp_filt <- data.frame(t(tmp_get_sim$filtX))
    tmp_filt <- tmp_filt %>% tibble::rownames_to_column("ID")
    tmp_tab <- data.frame(t(tmp_get))
    tmp_tab <- tmp_tab %>% tibble::rownames_to_column("ID")
    tmp_pvals <- data.frame(tmp_get_sim$pvals)
    tmp_pvals <- tmp_pvals %>% tibble::rownames_to_column("ID") %>% 
              dplyr::rename("pval" = 2)
    tmp_pvals <- tmp_pvals %>% filter(pval <= get(tmp_pval))
    tmp_merge <- dplyr::left_join(tmp_pvals, tmp_tab, by = "ID")
    tmp_merge[, 2] <- NULL
    tmp_merge <- tmp_merge %>% tibble::column_to_rownames("ID")
    tmp_tax <- data.frame(tax_table(get(i))) %>% tibble::rownames_to_column("ID")
    tmp_tax <- dplyr::left_join(tmp_pvals, tmp_tax, by = "ID")
    tmp_tax[, 2] <- NULL
    tmp_tax <- tmp_tax %>% tibble::column_to_rownames("ID")
# Build PS object
    tmp_samp <- data.frame(sample_data(get(i)))
    identical(row.names(tmp_tax), row.names(tmp_merge))
    tmp_merge <- data.frame(t(tmp_merge))
    tmp_merge <- as.matrix(tmp_merge)
    tmp_tax <- as.matrix(tmp_tax)
    tmp_ps <- phyloseq(otu_table(tmp_merge, taxa_are_rows = FALSE),
                     tax_table(tmp_tax),
                     sample_data(tmp_samp))
    
    tmp_tree <- rtree(ntaxa(tmp_ps), rooted = TRUE,
                           tip.label = taxa_names(tmp_ps))
    tmp_ps <- merge_phyloseq(tmp_ps,
                               sample_data,
                               tmp_tree)
    tmp_ps_name <- purrr::map_chr(i, ~ paste0(., "_perf_filt"))
    assign(tmp_ps_name, tmp_ps)
    rm(list = ls(pattern = "tmp_"))
}  

ssu_ps_perfect <- ssu_ps_work_perf_filt
its_ps_perfect <- its_ps_work_perf_filt
rm(list = ls(pattern = "_perf_filt"))
```

```{r, eval=TRUE, echo=FALSE}
print("16S rRNA phyloseq object", quote = FALSE)
ssu_ps_perfect
print("ITS phyloseq object", quote = FALSE)
its_ps_perfect
```

## Summary

How many reads and ASVs were removed following PERFect filtering?

**16S rRNA**

```{r, echo=FALSE}
## This code is gross. I spent hours trying to find a better
## solution but failed :(
rm(list = ls(pattern = "tmp_"))
tmp_samples <- c("ssu_ps_work", "ssu_ps_perfect")
tmp_objects <- c(
                "Total number of reads", 
                "Min. number of reads", 
                "Max. number of reads", 
                "Average number of reads", 
                "Median number of reads", 
                "Total ASVs", 
                "Min. number of ASVs", 
                "Max. number of ASVs", 
                "Average number of ASVs", 
                "Median number of ASVs")

tmp_total_reads <- c()
for (i in tmp_samples) {
   tmp_get <- sum(readcount(get(i)))
   tmp_total_reads <- c(append(tmp_total_reads, tmp_get))
}
tmp_min_read <- c()
for (i in tmp_samples) {
   tmp_get <- min(readcount(get(i)))
   tmp_min_read <- c(append(tmp_min_read, tmp_get))
}
tmp_max_read <- c()
for (i in tmp_samples) {
   tmp_get <- max(readcount(get(i)))
   tmp_max_read <- c(append(tmp_max_read, tmp_get))
}
tmp_mean_reads <- c()
for (i in tmp_samples) {
   tmp_get <- round(mean(readcount(get(i))), digits = 0)
   tmp_mean_reads <- c(append(tmp_mean_reads, tmp_get))
}
tmp_median_reads <- c()
for (i in tmp_samples) {
   tmp_get <- median(readcount(get(i)))
   tmp_median_reads <- c(append(tmp_median_reads, tmp_get))
}
tmp_total_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- ntaxa(get(i))
   tmp_total_asvs <- c(append(tmp_total_asvs, tmp_get))
}
tmp_min_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- min(richness(get(i), index = "observed"))
   tmp_min_asvs <- c(append(tmp_min_asvs, tmp_get))
}
tmp_max_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- max(richness(get(i), index = "observed"))
   tmp_max_asvs <- c(append(tmp_max_asvs, tmp_get))
}
tmp_mean_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- round(mean(richness(get(i), index = "observed")$observed), digits = 0)
   tmp_mean_asvs <- c(append(tmp_mean_asvs, tmp_get))
}
tmp_med_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- median(richness(get(i), index = "observed")$observed)
   tmp_med_asvs <- c(append(tmp_med_asvs, tmp_get))
}

tmp_bind <- data.frame(do.call("rbind", list(
  tmp_total_reads, tmp_min_read, tmp_max_read, tmp_mean_reads, tmp_median_reads, 
  tmp_total_asvs, tmp_min_asvs, tmp_max_asvs, tmp_mean_asvs, tmp_med_asvs)))

ssu_sum_perfect <- dplyr::bind_cols(tmp_objects, tmp_bind) %>%
  dplyr::rename("Metric" = 1, "Full data" = 2, "PERFect filter" = 3)
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_ssu("ssu_perfect_filt_summary")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- ssu_sum_perfect
seq_table %>%
  download_this(
    output_name = "ssu_perfect_filt_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = FALSE, sortable = FALSE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 150
    ), 
  columns = list(
    Metric = colDef(name = "Metric", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200)
    ), 
  searchable = FALSE, defaultPageSize = 10, showPagination = FALSE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = FALSE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

**ITS**

```{r, echo=FALSE}
rm(list = ls(pattern = "tmp_"))
tmp_samples <- c("its_ps_work", "its_ps_perfect")
tmp_objects <- c(
                "Total number of reads", 
                "Min. number of reads", 
                "Max. number of reads", 
                "Average number of reads", 
                "Median number of reads", 
                "Total ASVs", 
                "Min. number of ASVs", 
                "Max. number of ASVs", 
                "Average number of ASVs", 
                "Median number of ASVs")

tmp_total_reads <- c()
for (i in tmp_samples) {
   tmp_get <- sum(readcount(get(i)))
   tmp_total_reads <- c(append(tmp_total_reads, tmp_get))
}
tmp_min_read <- c()
for (i in tmp_samples) {
   tmp_get <- min(readcount(get(i)))
   tmp_min_read <- c(append(tmp_min_read, tmp_get))
}
tmp_max_read <- c()
for (i in tmp_samples) {
   tmp_get <- max(readcount(get(i)))
   tmp_max_read <- c(append(tmp_max_read, tmp_get))
}
tmp_mean_reads <- c()
for (i in tmp_samples) {
   tmp_get <- round(mean(readcount(get(i))), digits = 0)
   tmp_mean_reads <- c(append(tmp_mean_reads, tmp_get))
}
tmp_median_reads <- c()
for (i in tmp_samples) {
   tmp_get <- median(readcount(get(i)))
   tmp_median_reads <- c(append(tmp_median_reads, tmp_get))
}
tmp_total_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- ntaxa(get(i))
   tmp_total_asvs <- c(append(tmp_total_asvs, tmp_get))
}
tmp_min_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- min(richness(get(i), index = "observed"))
   tmp_min_asvs <- c(append(tmp_min_asvs, tmp_get))
}
tmp_max_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- max(richness(get(i), index = "observed"))
   tmp_max_asvs <- c(append(tmp_max_asvs, tmp_get))
}
tmp_mean_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- round(mean(richness(get(i), index = "observed")$observed), digits = 0)
   tmp_mean_asvs <- c(append(tmp_mean_asvs, tmp_get))
}
tmp_med_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- median(richness(get(i), index = "observed")$observed)
   tmp_med_asvs <- c(append(tmp_med_asvs, tmp_get))
}

tmp_bind <- data.frame(do.call("rbind", list(
  tmp_total_reads, tmp_min_read, tmp_max_read, tmp_mean_reads, tmp_median_reads, 
  tmp_total_asvs, tmp_min_asvs, tmp_max_asvs, tmp_mean_asvs, tmp_med_asvs)))

its_sum_perfect <- dplyr::bind_cols(tmp_objects, tmp_bind) %>%
  dplyr::rename("Metric" = 1, "Full data" = 2, "PERFect filter" = 3)
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_its("its_perfect_filt_summary")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- its_sum_perfect
seq_table %>%
  download_this(
    output_name = "its_perfect_filt_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = FALSE, sortable = FALSE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 150
    ), 
  columns = list(
    Metric = colDef(name = "Metric", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200)
    ), 
  searchable = FALSE, defaultPageSize = 10, showPagination = FALSE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = FALSE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

Here is a summary table of total reads and ASVs on a per sample basis after filtering. 

**16S rRNA**

```{r, echo=FALSE}
tmp_read_count <- data.frame(sample_sums(ssu_ps_perfect)) %>%
  tibble::rownames_to_column("SamName")
tmp_asv_count <- estimate_richness(ssu_ps_perfect, split = TRUE, measures = "Observed") %>%
  tibble::rownames_to_column("SamName")
tmp_samp_data <- data.frame(sample_data(ssu_ps_perfect))
tmp_samp_data <- tmp_samp_data[order(tmp_samp_data$LOC, tmp_samp_data$YEAR),]

tmp_join <- dplyr::left_join(tmp_samp_data, tmp_read_count, by = "SamName") %>%
  dplyr::left_join(., tmp_asv_count, by = "SamName") %>%
  dplyr::rename("Sample_ID" = 1) %>%
  dplyr::rename("total_reads" = 8) %>%
  dplyr::rename("total_asvs" = 9)
ssu_perfect_samp_summary <- tmp_join
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_ssu("ssu_perfect_samp_summary")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- ssu_perfect_samp_summary
seq_table %>%
  download_this(
    output_name = "ssu_perfect_samp_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-page", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = TRUE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    Sample_ID = colDef(name = "Sample_ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200),
     total_reads = colDef(name = "total reads", filterable = FALSE, minWidth = 140),
     total_asvs = colDef(name = "total asvs", filterable = FALSE, minWidth = 140)
), 
  searchable = TRUE, defaultPageSize = 5, showPagination = TRUE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

**ITS**

```{r, echo=FALSE}
tmp_read_count <- data.frame(sample_sums(its_ps_perfect)) %>%
  tibble::rownames_to_column("SamName")
tmp_asv_count <- estimate_richness(its_ps_perfect, split = TRUE, measures = "Observed") %>%
  tibble::rownames_to_column("SamName")
tmp_samp_data <- data.frame(sample_data(its_ps_perfect))
tmp_samp_data <- tmp_samp_data[order(tmp_samp_data$LOC, tmp_samp_data$YEAR),]

tmp_join <- dplyr::left_join(tmp_samp_data, tmp_read_count, by = "SamName") %>%
  dplyr::left_join(., tmp_asv_count, by = "SamName") %>%
  dplyr::rename("Sample_ID" = 1) %>%
  dplyr::rename("total_reads" = 8) %>%
  dplyr::rename("total_asvs" = 9)
its_perfect_samp_summary <- tmp_join
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_its("its_perfect_samp_summary")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- its_perfect_samp_summary
seq_table %>%
  download_this(
    output_name = "its_perfect_samp_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-page", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = TRUE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    Sample_ID = colDef(name = "Sample_ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200),
     total_reads = colDef(name = "total reads", filterable = FALSE, minWidth = 140),
     total_asvs = colDef(name = "total asvs", filterable = FALSE, minWidth = 140)
    ), 
  searchable = TRUE, defaultPageSize = 5, showPagination = TRUE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

And finally, save the PERFect filtered phyloseq objects.

```{r, code_folding=TRUE}
saveRDS(ssu_ps_perfect, "files/filtering/perfect/rdata/ssu_ps_perfect.rds")
saveRDS(its_ps_perfect, "files/filtering/perfect/rdata/its_ps_perfect.rds")
```

```{r, echo=FALSE}
save.image("page_build/filtering_wf.rdata")
```

# C. PIME Filtering

Unlike the two previous workflows---which combined the analyses of the 16S rRNA and ITS data sets---here the two data sets will be analyzed separately because the PIME workflow is considerably more complicated.

## 16S rRNA

```{r, echo=FALSE, eval=TRUE}
## FOR page build #3
gdata::keep(caption_fig_its, caption_fig_ssu, 
            caption_tab_its, caption_tab_ssu, 
            ref, sure = TRUE)
source("assets/captions/captions_filtering.R")
load("page_build/filtering_pime_ssu_wf.rdata")
```

```{r, echo=FALSE, eval=FALSE}
## After PIME analysis complete #2
remove(list = ls())
load("files/filtering/pime/rdata/pime_ssu_asv_b4_rand.rdata")
objects()
```

```{r, echo=FALSE}
## Initial Load for PIME  ANALYSIS #1
remove(list = ls())
set.seed(119)
ssu_ps_work <- readRDS("files/data-prep/rdata/ssu_ps_work.rds")
```

### Setup

First, choose a phyloseq object.

```{r, code_folding=TRUE}
ssu_pime_ds <- ssu_ps_work
ssu_which_pime <- "ssu_pime_ds"
ssu_pime_ds@phy_tree <- NULL
ssu_pime_ds
```

```{r, echo=FALSE, eval=TRUE}
tmp_ps <- ssu_ps_work
tmp_ps@phy_tree <- NULL
tmp_ps
```

Next, we add a new sample variable that combines the sample year and the treatment. We will use this new variable to split the data set for PIME analysis. 

```{r, code_folding=TRUE}
tmp_samp_data <- sample_data(ssu_pime_ds)	
tmp_samp_data$YR_TREAT <- paste(tmp_samp_data$YEAR, tmp_samp_data$TREAT, sep="_")
tmp_ps <- merge_phyloseq(ssu_pime_ds, tmp_samp_data)	
ssu_pime_ds <- tmp_ps	
rm(list = ls(pattern = "tmp_"))
```

Per the developers recommendation, the first step in the PIME process is to rarefy the data and then proceed with the filtering. The minimum read count in this data set is **`r min(readcount(ssu_ps_work))``** reads. 

```{r, code_folding=TRUE}
ssu_pime_sample_d <- data.frame(rowSums(otu_table(ssu_pime_ds)))
ssu_pime_sample_d <- ssu_pime_sample_d %>% dplyr::rename(total_reads = 1)

ssu_pime_ds <- rarefy_even_depth(
                  ssu_pime_ds, 
                  sample.size = min(ssu_pime_sample_d$total_reads), 
                  trimOTUs = TRUE, replace = FALSE, rngseed = 119)
```

```
`set.seed(119)` was used to initialize repeatable random subsampling.
Please record this for your records so others can reproduce.
Try `set.seed(119); .Random.seed` for the full vector

3737 OTUs were removed because they are no longer 
present in any sample after random subsampling
```

```{r, echo=FALSE, eval=TRUE}
ssu_pime_ds
```

The first step in PIME is to define if the microbial community presents a high relative abundance of taxa with low prevalence, which is considered as noise in PIME analysis. This is calculated by random forests analysis and is the baseline noise detection.

```{r, code_folding=TRUE}
ssu_pime.oob.error <- pime.oob.error(ssu_pime_ds, "YR_TREAT")
```

```{r, echo=FALSE, eval=TRUE}
ssu_pime.oob.error
```

### Split by Predictor Variable

```{r, code_folding=TRUE}
data.frame(sample_data(ssu_pime_ds))
ssu_per_variable_obj <- pime.split.by.variable(ssu_pime_ds, "YR_TREAT")
ssu_per_variable_obj
```

<details markdown="1">
<summary>**Detailed results** for each split by predictor variable</summary>

```{r, echo=FALSE, eval=TRUE}
ssu_per_variable_obj
```
</details>

### Calculate Prevalence Intervals

Using the output of `pime.split.by.variable`, we calculate the prevalence intervals with the function `pime.prevalence`. This function estimates the highest prevalence possible (no empty ASV table), calculates prevalence for taxa, starting at 5 maximum prevalence possible (no empty ASV table or dropping samples). After prevalence calculation, each prevalence interval are merged.

```{r, code_folding=TRUE}
ssu_prevalences <- pime.prevalence(ssu_per_variable_obj)
ssu_prevalences
```

<details markdown="1">
<summary>**Detailed results** for all prevalence intervals  </summary>

```{r, echo=FALSE, eval=TRUE}
ssu_prevalences
```
</details>

### Calculate Best Prevalence

Finally, we use the function `pime.best.prevalence` to calculate the best prevalence. The function uses randomForest to build random forests trees for samples classification and variable importance computation. It performs classifications for each prevalence interval returned by `pime.prevalence`. Variable importance is calculated, returning the Mean Decrease Accuracy (MDA), Mean Decrease Impurity (MDI), overall and by sample group, and taxonomy for each ASV. PIME keeps the top 30 variables with highest MDA each prevalence level.

```{r, code_folding=TRUE}
set.seed(1911)
ssu_best.prev <- pime.best.prevalence(ssu_prevalences, "YR_TREAT")
```

```{r, echo=FALSE, eval=FALSE}
ssu_best.prev$`OOB error`
```

<small>`r caption_tab_ssu("ssu_pime_oob_best_prev")`</small>
```{r, echo=FALSE, eval=TRUE}
seq_table <- ssu_best.prev$`OOB error`
seq_table %>%
  download_this(
    output_name = "ssu_pime_oob_best_prev",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 4),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    Interval = colDef(name = "Interval", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 100),
    OTUs = colDef(name = "ASVs"),
    Nseqs = colDef(name = "no. reads")
    ), 
  searchable = FALSE, defaultPageSize = 19, showPagination = FALSE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = FALSE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = FALSE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

```{r, code_folding=TRUE}
ssu_what_is_best <- ssu_best.prev$`OOB error`
ssu_what_is_best[, c(2:4)] <- sapply(ssu_what_is_best[, c(2:4)], as.numeric)
ssu_what_is_best <- ssu_what_is_best %>%
  dplyr::rename("OOB_error_rate" = "OOB error rate (%)")
ssu_what_is_best$Interval <- str_replace_all(ssu_what_is_best$Interval, "%", "")
ssu_best <- with(ssu_what_is_best, Interval[which.min(OOB_error_rate)])
ssu_best <- paste("`", ssu_best, "`", sep = "")
ssu_prev_choice <- paste("ssu_best.prev$`Importance`$", ssu_best, sep = "")
ssu_imp_best <- eval(parse(text = (ssu_prev_choice)))
```

Looks like the lowest OOB error rate (%) that retains the most ASVs is `r min(ssu_what_is_best$OOB_error_rate)`% from `r ssu_best`. We will use this interval.

### Best Prevalence Summary

```{r, echo=FALSE}
ssu_imp_best[,(ncol(ssu_imp_best)-1):ncol(ssu_imp_best)] <- list(NULL)
ssu_imp_best <- ssu_imp_best %>% dplyr::rename("ASV_ID" = "SequenceID")
```

<small>`r caption_tab_ssu("ssu_pime_filt_top_asvs")`</small>
```{r, echo=FALSE, eval=TRUE}
seq_table <- ssu_imp_best
seq_table %>%
  download_this(
    output_name = "ssu_pime_filt_top_asvs",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body-outset", eval=TRUE}
seq_table <- ssu_imp_best %>% 
 mutate_if(is.numeric, round, digits = 4)

reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 4),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    ASV_ID = colDef(name = "ASV ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 100, filterable = TRUE),
    Kingdom = colDef(align = "left", filterable = TRUE),
    Phylum = colDef(align = "left", filterable = TRUE, minWidth = 150),
    Class = colDef(align = "left", filterable = TRUE, minWidth = 150),
    Order = colDef(align = "left", filterable = TRUE, minWidth = 150),
    Family = colDef(align = "left", filterable = TRUE, minWidth = 150),
    Genus = colDef(align = "left", filterable = TRUE, minWidth = 150)
    ), 
  searchable = FALSE, defaultPageSize = 5, showPagination = TRUE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

Here is the list of the top 30 ASVs.

```{r, eval=TRUE, echo=FALSE}
ssu_imp_best$ASV_ID
```

Now we need to create a phyloseq object of ASVs at this cutoff (`r ssu_best`).

```{r}
ssu_best_val <- str_replace_all(ssu_best, "Prevalence ", "")
ssu_best_val <- paste("ssu_prevalences$", ssu_best_val, sep = "")
ssu_prevalence_best <- eval(parse(text = (ssu_best_val)))
saveRDS(ssu_prevalence_best, "files/filtering/pime/rdata/ssu_prevalence_best.rds")
```

And look at a summary of the phyloseq object.

```{r, echo=FALSE, eval=TRUE}
ssu_prevalence_best
```

```{r, echo=FALSE}
rm(list = ls(pattern = "tmp_"))
tmp_samples <- c("ssu_ps_work", "ssu_prevalence_best")
tmp_objects <- c(
                "Total number of reads", 
                "Min. number of reads", 
                "Max. number of reads", 
                "Average number of reads", 
                "Median number of reads", 
                "Total ASVs", 
                "Min. number of ASVs", 
                "Max. number of ASVs", 
                "Average number of ASVs", 
                "Median number of ASVs")

tmp_total_reads <- c()
for (i in tmp_samples) {
   tmp_get <- sum(readcount(get(i)))
   tmp_total_reads <- c(append(tmp_total_reads, tmp_get))
}
tmp_min_read <- c()
for (i in tmp_samples) {
   tmp_get <- min(readcount(get(i)))
   tmp_min_read <- c(append(tmp_min_read, tmp_get))
}
tmp_max_read <- c()
for (i in tmp_samples) {
   tmp_get <- max(readcount(get(i)))
   tmp_max_read <- c(append(tmp_max_read, tmp_get))
}
tmp_mean_reads <- c()
for (i in tmp_samples) {
   tmp_get <- round(mean(readcount(get(i))), digits = 0)
   tmp_mean_reads <- c(append(tmp_mean_reads, tmp_get))
}
tmp_median_reads <- c()
for (i in tmp_samples) {
   tmp_get <- median(readcount(get(i)))
   tmp_median_reads <- c(append(tmp_median_reads, tmp_get))
}
tmp_total_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- ntaxa(get(i))
   tmp_total_asvs <- c(append(tmp_total_asvs, tmp_get))
}
tmp_min_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- min(richness(get(i), index = "observed"))
   tmp_min_asvs <- c(append(tmp_min_asvs, tmp_get))
}
tmp_max_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- max(richness(get(i), index = "observed"))
   tmp_max_asvs <- c(append(tmp_max_asvs, tmp_get))
}
tmp_mean_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- round(mean(richness(get(i), index = "observed")$observed), digits = 0)
   tmp_mean_asvs <- c(append(tmp_mean_asvs, tmp_get))
}
tmp_med_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- median(richness(get(i), index = "observed")$observed)
   tmp_med_asvs <- c(append(tmp_med_asvs, tmp_get))
}

tmp_bind <- data.frame(do.call("rbind", list(
  tmp_total_reads, tmp_min_read, tmp_max_read, tmp_mean_reads, tmp_median_reads, 
  tmp_total_asvs, tmp_min_asvs, tmp_max_asvs, tmp_mean_asvs, tmp_med_asvs)))

ssu_sum_pime <- dplyr::bind_cols(tmp_objects, tmp_bind) %>%
  dplyr::rename("Metric" = 1, "Full data" = 2, "PIME filter" = 3)
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_ssu("ssu_pime_filt_summary")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- ssu_sum_pime
seq_table %>%
  download_this(
    output_name = "ssu_pime_filt_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 150
    ), 
  columns = list(
    Metric = colDef(name = "Metric", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200)
    ), 
  searchable = FALSE, defaultPageSize = 10, showPagination = FALSE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = FALSE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

### Estimate Error in Prediction

Using  the function `pime.error.prediction` we can estimate the error in prediction. For each prevalence interval, this function randomizes the samples labels into arbitrary groupings using `n` random permutations, defined by the `bootstrap` value. For each, randomized and prevalence filtered, data set the OOB error rate is calculated to estimate whether the original differences in groups of samples occur by chance. Results are in a list containing a table and a box plot summarizing the results.

> This step takes a **really** long time to run. The code is included below in case you would like to perform this analysis.

<details markdown="1">
<summary>**Code** to Estimate Error in Prediction</summary>
```{r}
ssu_randomized <- pime.error.prediction(ssu_pime_ds, "TEMP",
                                          bootstrap = 100, parallel = TRUE,
                                          max.prev = 95)
```

```{r, echo=FALSE}
ssu_oob_error <- ssu_randomized$`Results table`
```

<small>`r caption_tab_ssu("ssu_pime_est_error")`</small>

```{r, echo=FALSE, eval=FALSE}
seq_table <- ssu_oob_error
seq_table %>%
  download_this(
    output_name = "ssu_pime_est_error",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body-outset", eval=FALSE}
seq_table <- ssu_oob_error  %>% 
 mutate_if(is.numeric, round, digits = 4)

reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 4),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 120
    ), 
  searchable = FALSE, defaultPageSize = 5, showPagination = TRUE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

```{r, echo=FALSE}
ssu_randomized$Plot
```
</details>

It is also possible to estimate the variation of OOB error for each prevalence interval filtering. This is done by running the random forests classification for `n` times, determined by the `bootstrap` value. The function will return a box plot figure and a table for each classification error.

```{r, code_folding=TRUE}
ssu_replicated.oob.error <- pime.oob.replicate(ssu_prevalences, "TEMP",
                                         bootstrap = 100, parallel = TRUE)
```

```{r, echo=FALSE, eval=TRUE, fig.height=3}
ssu_obb_orig <- ssu_replicated.oob.error$Plot
ssu_obb_orig <- ssu_obb_orig +
  theme() +
  labs(title = "OOB error Original data set")
ssu_obb_orig
```
<small>`r caption_fig_ssu("ssu_pime_obb_plot")`</small>

```{r, echo=FALSE, eval=FALSE, fig.height=5}
## set eval=TRUE TO include random plot
## and remove previous chunk

ssu_obb_orig <- ssu_replicated.oob.error$Plot
ssu_obb_rand <- ssu_randomized$Plot

ssu_obb_orig <- ssu_obb_orig +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank()) +
  labs(title = "OOB error Original data set")

ssu_obb_rand <- ssu_obb_rand  +
  labs(title = "OOB error Randomized data set")
ssu_obb_orig / ssu_obb_rand
```

To obtain the confusion matrix from random forests classification use the following:

```{r, code_folding=TRUE}
ssu_prev_confuse <- paste("ssu_best.prev$`Confusion`$", ssu_best, sep = "")
eval(parse(text = (ssu_prev_confuse)))
```

### Save Phyloseq PIME objects

Here we save a PIME filtered phyloseq object and add a tree.

```{r, code_folding=TRUE}
ssu_ps_pime <- ssu_prevalence_best
ssu_ps_pime_tree <- rtree(ntaxa(ssu_ps_pime), rooted = TRUE,
                           tip.label = taxa_names(ssu_ps_pime))
ssu_ps_pime <- merge_phyloseq(ssu_ps_pime,
                               sample_data,
                               ssu_ps_pime_tree)
saveRDS(ssu_ps_pime, "files/filtering/pime/rdata/ssu_ps_pime.rds")
```

### Split & save by predictor variable

```{r, code_folding=TRUE}
ssu_ps_pime_split <- pime.split.by.variable(ssu_ps_pime, "YR_TREAT")
saveRDS(ssu_ps_pime_split$Y0_CTL, "files/filtering/pime/rdata/ssu_ps_pime_Y0_CTL.rds")
saveRDS(ssu_ps_pime_split$Y0_N, "files/filtering/pime/rdata/ssu_ps_pime_Y0_N.rds")
saveRDS(ssu_ps_pime_split$Y0_NP, "files/filtering/pime/rdata/ssu_ps_pime_Y0_NP.rds")
saveRDS(ssu_ps_pime_split$Y0_P, "files/filtering/pime/rdata/ssu_ps_pime_Y0_P.rds")
saveRDS(ssu_ps_pime_split$Y1_CTL, "files/filtering/pime/rdata/ssu_ps_pime_Y1_CTL.rds")
saveRDS(ssu_ps_pime_split$Y1_N, "files/filtering/pime/rdata/ssu_ps_pime_Y1_N.rds")
saveRDS(ssu_ps_pime_split$Y1_NP, "files/filtering/pime/rdata/ssu_ps_pime_Y1_NP.rds")
saveRDS(ssu_ps_pime_split$Y1_P, "files/filtering/pime/rdata/ssu_ps_pime_Y1_P.rds")
saveRDS(ssu_ps_pime_split$Y4_CTL, "files/filtering/pime/rdata/ssu_ps_pime_Y4_CTL.rds")
saveRDS(ssu_ps_pime_split$Y4_N, "files/filtering/pime/rdata/ssu_ps_pime_Y4_N.rds")
saveRDS(ssu_ps_pime_split$Y4_NP, "files/filtering/pime/rdata/ssu_ps_pime_Y4_NP.rds")
saveRDS(ssu_ps_pime_split$Y4_P, "files/filtering/pime/rdata/ssu_ps_pime_Y4_P.rds")
```

<details markdown="1">
<summary>**Phyloseq objects** for each split</summary>
```{r, echo=FALSE, eval=TRUE}
ssu_ps_pime_split
```
</details>

```{r, echo=FALSE}
ssu_pime_preval.tax <- tax_table(ssu_ps_pime)
write.table(ssu_pime_preval.tax,
            file="files/filtering/pime/tables/ssu_asv_PIME_tax_table.txt",
            sep = "\t", quote = FALSE)

ssu_pime_preval.asv <- otu_table(t(ssu_ps_pime))
write.table(ssu_pime_preval.asv,
            file="files/filtering/pime/tables/ssu_asv_PIME_otu_table.txt",
            sep = "\t", quote = FALSE)

ssu_pime_preval.samp <- sample_data(ssu_ps_pime)
write.table(ssu_pime_preval.samp,
            file="files/filtering/pime/tables/ssu_asv_PIME_sample_data.txt",
            sep = "\t", quote = FALSE)
```

### Summary

You know the routine. How did filtering affect total reads and ASVs for each sample?

```{r, echo=FALSE}
tmp_read_count <- data.frame(sample_sums(ssu_ps_pime)) %>%
  tibble::rownames_to_column("SamName")
tmp_asv_count <- estimate_richness(ssu_ps_pime, split = TRUE, measures = "Observed") %>%
  tibble::rownames_to_column("SamName")
tmp_samp_data <- data.frame(sample_data(ssu_ps_pime))
tmp_samp_data <- tmp_samp_data[order(tmp_samp_data$YEAR, tmp_samp_data$TREAT),]

tmp_join <- dplyr::left_join(tmp_samp_data, tmp_read_count, by = "SamName") %>%
  dplyr::left_join(., tmp_asv_count, by = "SamName") %>%
  dplyr::rename("Sample_ID" = 1) %>%
  dplyr::rename("total_reads" = 9) %>%
  dplyr::rename("total_asvs" = 10)
ssu_pime_samp_summary <- tmp_join
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_ssu("ssu_pime_samp_summary")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- ssu_pime_samp_summary
seq_table %>%
  download_this(
    output_name = "ssu_pime_samp_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-page", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = TRUE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    Sample_ID = colDef(name = "Metric", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200),
     total_reads = colDef(name = "total reads", filterable = FALSE, minWidth = 140),
     total_asvs = colDef(name = "total asvs", filterable = FALSE, minWidth = 140)
    ), 
  searchable = TRUE, defaultPageSize = 5, showPagination = TRUE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

And here is how the subsets changed through the PIME filtering process.

```{r, echo=FALSE, eval=TRUE}
ssu_ps_pime_Y0_CTL <- ssu_ps_pime_split$`Y0_CTL`
ssu_ps_pime_Y0_N <- ssu_ps_pime_split$`Y0_N`
ssu_ps_pime_Y0_NP <- ssu_ps_pime_split$`Y0_NP`
ssu_ps_pime_Y0_P <- ssu_ps_pime_split$`Y0_P`
ssu_ps_pime_Y1_CTL <- ssu_ps_pime_split$`Y1_CTL`
ssu_ps_pime_Y1_N <- ssu_ps_pime_split$`Y1_N`
ssu_ps_pime_Y1_NP <- ssu_ps_pime_split$`Y1_NP`
ssu_ps_pime_Y1_P <- ssu_ps_pime_split$`Y1_P`
ssu_ps_pime_Y4_CTL <- ssu_ps_pime_split$`Y4_CTL`
ssu_ps_pime_Y4_N <- ssu_ps_pime_split$`Y4_N`
ssu_ps_pime_Y4_NP <- ssu_ps_pime_split$`Y4_NP`
ssu_ps_pime_Y4_P <- ssu_ps_pime_split$`Y4_P`

tmp_objects <- data.frame(c("FULL data set", "Rarefied data", "PIME filtered data",
                            "PIME (Y0_CTL)", "PIME (Y0_N)", "PIME (Y0_NP)", 
                            "PIME (Y0_P)", "PIME (Y1_CTL)", "PIME (Y1_N)", 
                            "PIME (Y1_NP)", "PIME (Y1_P)", "PIME (Y4_CTL)", 
                            "PIME (Y4_N)", "PIME (Y4_NP)", "PIME (Y4_P)"))
                            
                            
tmp_samples <- c("ssu_ps_work", "ssu_pime_ds", "ssu_ps_pime",
                 "ssu_ps_pime_Y0_CTL", "ssu_ps_pime_Y0_N", "ssu_ps_pime_Y0_NP", 
                 "ssu_ps_pime_Y0_P", "ssu_ps_pime_Y1_CTL", "ssu_ps_pime_Y1_N", 
                 "ssu_ps_pime_Y1_NP", "ssu_ps_pime_Y1_P", "ssu_ps_pime_Y4_CTL", 
                 "ssu_ps_pime_Y4_N", "ssu_ps_pime_Y4_NP", "ssu_ps_pime_Y4_P")
tmp_no_samp <- c()
for (i in tmp_samples) {
   tmp_get <- nsamples(get(i))
   tmp_no_samp <- c(append(tmp_no_samp, tmp_get))
}
tmp_no_samp <- data.frame(tmp_no_samp)

tmp_rc <- c()
for (i in tmp_samples) {
   tmp_get <- sum(readcount(get(i)))
   tmp_rc <- c(append(tmp_rc, tmp_get))
}
tmp_rc <- data.frame(tmp_rc)
tmp_asv <- c()
for (i in tmp_samples) {
   tmp_get <- ntaxa(get(i))
   tmp_asv <- c(append(tmp_asv, tmp_get))
}
tmp_asv <- data.frame(tmp_asv)

ssu_asv_pime_sum <- dplyr::bind_cols(tmp_objects, tmp_no_samp) %>%
                      dplyr::bind_cols(., tmp_rc) %>%
                      dplyr::bind_cols(., tmp_asv) %>%
  dplyr::rename("Description" = 1, "no. samples" = 2,
                "total reads" = 3, "total asvs" = 4)
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_ssu("ssu_asv_pime_sum")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- ssu_asv_pime_sum
seq_table %>%
  download_this(
    output_name = "ssu_asv_pime_sum",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = FALSE, sortable = FALSE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    Description = colDef(name = "Description", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 150)
    ), 
  searchable = FALSE, defaultPageSize = 15, showPagination = FALSE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = FALSE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```


```{r, echo=FALSE}
save.image("page_build/pime_ssu_asv_wf.rdata")
objects()
```

## ITS

```{r, echo=FALSE, eval=TRUE}
## FOR page build #3
gdata::keep(caption_fig_its, caption_fig_ssu, 
            caption_tab_its, caption_tab_ssu, 
            ref, sure = TRUE)
source("assets/captions/captions_filtering.R")
load("page_build/filtering_pime_its_wf.rdata")
```

```{r, echo=FALSE, eval=FALSE}
## After PIME analysis complete #2
remove(list = ls())
load("files/filtering/pime/rdata/pime_its_asv_b4_rand.rdata")
objects()
```

```{r, echo=FALSE}
## Initial Load for PIME  ANALYSIS #1
remove(list = ls())
set.seed(119)
its_ps_work <- readRDS("files/data-prep/rdata/its_ps_work.rds")
```

```{r, echo=FALSE}
## Initial Load for PIME  ANALYSIS #1
remove(list = ls())
set.seed(119)
its_ps_work <- readRDS("files/data-prep/rdata/its_ps_work.rds")
```

### Setup

First, choose a phyloseq object.

```{r, code_folding=TRUE}
its_pime_ds <- its_ps_work
its_which_pime <- "its_pime_ds"
its_pime_ds@phy_tree <- NULL
its_pime_ds
```

```{r, echo=FALSE, eval=TRUE}
tmp_ps <- its_ps_work
tmp_ps@phy_tree <- NULL
tmp_ps
```

Next, we add a new sample variable that combines the sample year and the treatment. We will use this new variable to split the data set for PIME analysis. 

```{r, code_folding=TRUE}
tmp_samp_data <- sample_data(its_pime_ds)	
tmp_samp_data$YR_TREAT <- paste(tmp_samp_data$YEAR, tmp_samp_data$TREAT, sep="_")
tmp_ps <- merge_phyloseq(its_pime_ds, tmp_samp_data)	
its_pime_ds <- tmp_ps	
rm(list = ls(pattern = "tmp_"))
```

Per the developers recommendation, the first step in the PIME process is to rarefy the data and then proceed with the filtering. The minimum read count in this data set is **`r min(readcount(its_ps_work))``** reads. 

```{r, code_folding=TRUE}
its_pime_sample_d <- data.frame(rowSums(otu_table(its_pime_ds)))
its_pime_sample_d <- its_pime_sample_d %>% dplyr::rename(total_reads = 1)

its_pime_ds <- rarefy_even_depth(
                  its_pime_ds, 
                  sample.size = min(its_pime_sample_d$total_reads), 
                  trimOTUs = TRUE, replace = FALSE, rngseed = 119)
```

```
`set.seed(119)` was used to initialize repeatable random subsampling.
Please record this for your records so others can reproduce.
Try `set.seed(119); .Random.seed` for the full vector

4081O OTUs were removed because they are no longer 
present in any sample after random subsampling
```

```{r, echo=FALSE, eval=TRUE}
its_pime_ds
```

The first step in PIME is to define if the microbial community presents a high relative abundance of taxa with low prevalence, which is considered as noise in PIME analysis. This is calculated by random forests analysis and is the baseline noise detection.

```{r, code_folding=TRUE}
its_pime.oob.error <- pime.oob.error(its_pime_ds, "YR_TREAT")
```

```{r, echo=FALSE, eval=TRUE}
its_pime.oob.error
```

### Split by Predictor Variable

```{r, code_folding=TRUE}
data.frame(sample_data(its_pime_ds))
its_per_variable_obj <- pime.split.by.variable(its_pime_ds, "YR_TREAT")
its_per_variable_obj
```

<details markdown="1">
<summary>**Detailed results** for each split by predictor variable</summary>

```{r, echo=FALSE, eval=TRUE}
its_per_variable_obj
```
</details>

### Calculate Prevalence Intervals

Using the output of `pime.split.by.variable`, we calculate the prevalence intervals with the function `pime.prevalence`. This function estimates the highest prevalence possible (no empty ASV table), calculates prevalence for taxa, starting at 5 maximum prevalence possible (no empty ASV table or dropping samples). After prevalence calculation, each prevalence interval are merged.

```{r, code_folding=TRUE}
its_prevalences <- pime.prevalence(its_per_variable_obj)
its_prevalences
```

<details markdown="1">
<summary>**Detailed results** for all prevalence intervals  </summary>

```{r, echo=FALSE, eval=TRUE}
its_prevalences
```
</details>

### Calculate Best Prevalence

Finally, we use the function `pime.best.prevalence` to calculate the best prevalence. The function uses randomForest to build random forests trees for samples classification and variable importance computation. It performs classifications for each prevalence interval returned by `pime.prevalence`. Variable importance is calculated, returning the Mean Decrease Accuracy (MDA), Mean Decrease Impurity (MDI), overall and by sample group, and taxonomy for each ASV. PIME keeps the top 30 variables with highest MDA each prevalence level.

```{r, code_folding=TRUE}
set.seed(1911)
its_best.prev <- pime.best.prevalence(its_prevalences, "YR_TREAT")
```

```{r, echo=FALSE, eval=FALSE}
its_best.prev$`OOB error`
```

<small>`r caption_tab_its("its_pime_oob_best_prev")`</small>
```{r, echo=FALSE, eval=TRUE}
seq_table <- its_best.prev$`OOB error`
seq_table %>%
  download_this(
    output_name = "its_pime_oob_best_prev",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 4),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    Interval = colDef(name = "Interval", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 100),
    OTUs = colDef(name = "ASVs"),
    Nseqs = colDef(name = "no. reads")
    ), 
  searchable = FALSE, defaultPageSize = 19, showPagination = FALSE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = FALSE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = FALSE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

```{r, code_folding=TRUE}
its_what_is_best <- its_best.prev$`OOB error`
its_what_is_best[, c(2:4)] <- sapply(its_what_is_best[, c(2:4)], as.numeric)
its_what_is_best <- its_what_is_best %>%
  dplyr::rename("OOB_error_rate" = "OOB error rate (%)")
its_what_is_best$Interval <- str_replace_all(its_what_is_best$Interval, "%", "")
its_best <- with(its_what_is_best, Interval[which.min(OOB_error_rate)])
its_best <- paste("`", its_best, "`", sep = "")
its_prev_choice <- paste("its_best.prev$`Importance`$", its_best, sep = "")
its_imp_best <- eval(parse(text = (its_prev_choice)))
```

Looks like the lowest OOB error rate (%) that retains the most ASVs is **`r min(its_what_is_best$OOB_error_rate)`%** from `r its_best`. We will use this interval.

### Best Prevalence Summary

```{r, echo=FALSE}
its_imp_best[,(ncol(its_imp_best)-1):ncol(its_imp_best)] <- list(NULL)
its_imp_best <- its_imp_best %>% dplyr::rename("ASV_ID" = "SequenceID")
```

<small>`r caption_tab_its("its_pime_filt_top_asvs")`</small>
```{r, echo=FALSE, eval=TRUE}
seq_table <- its_imp_best
seq_table %>%
  download_this(
    output_name = "its_pime_filt_top_asvs",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body-outset", eval=TRUE}
seq_table <- its_imp_best %>% 
 mutate_if(is.numeric, round, digits = 4)

reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 4),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    ASV_ID = colDef(name = "ASV ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 100, filterable = TRUE),
    Kingdom = colDef(align = "left", filterable = TRUE),
    Phylum = colDef(align = "left", filterable = TRUE, minWidth = 150),
    Class = colDef(align = "left", filterable = TRUE, minWidth = 150),
    Order = colDef(align = "left", filterable = TRUE, minWidth = 150),
    Family = colDef(align = "left", filterable = TRUE, minWidth = 150),
    Genus = colDef(align = "left", filterable = TRUE, minWidth = 150)
    ), 
  searchable = FALSE, defaultPageSize = 5, showPagination = TRUE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

Here is the list of the top 30 ASVs.

```{r, eval=TRUE, echo=FALSE}
its_imp_best$ASV_ID
```

Now we need to create a phyloseq object of ASVs at this cutoff (`r its_best`).

```{r, code_folding=TRUE}
its_best_val <- str_replace_all(its_best, "Prevalence ", "")
its_best_val <- paste("its_prevalences$", its_best_val, sep = "")
its_prevalence_best <- eval(parse(text = (its_best_val)))
saveRDS(its_prevalence_best, "files/filtering/pime/rdata/its_prevalence_best.rds")
```

And look at a summary of the phyloseq object.

```{r, echo=FALSE, eval=TRUE}
its_prevalence_best
```

```{r, echo=FALSE}
rm(list = ls(pattern = "tmp_"))
tmp_samples <- c("its_ps_work", "its_prevalence_best")
tmp_objects <- c(
                "Total number of reads", 
                "Min. number of reads", 
                "Max. number of reads", 
                "Average number of reads", 
                "Median number of reads", 
                "Total ASVs", 
                "Min. number of ASVs", 
                "Max. number of ASVs", 
                "Average number of ASVs", 
                "Median number of ASVs")

tmp_total_reads <- c()
for (i in tmp_samples) {
   tmp_get <- sum(readcount(get(i)))
   tmp_total_reads <- c(append(tmp_total_reads, tmp_get))
}
tmp_min_read <- c()
for (i in tmp_samples) {
   tmp_get <- min(readcount(get(i)))
   tmp_min_read <- c(append(tmp_min_read, tmp_get))
}
tmp_max_read <- c()
for (i in tmp_samples) {
   tmp_get <- max(readcount(get(i)))
   tmp_max_read <- c(append(tmp_max_read, tmp_get))
}
tmp_mean_reads <- c()
for (i in tmp_samples) {
   tmp_get <- round(mean(readcount(get(i))), digits = 0)
   tmp_mean_reads <- c(append(tmp_mean_reads, tmp_get))
}
tmp_median_reads <- c()
for (i in tmp_samples) {
   tmp_get <- median(readcount(get(i)))
   tmp_median_reads <- c(append(tmp_median_reads, tmp_get))
}
tmp_total_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- ntaxa(get(i))
   tmp_total_asvs <- c(append(tmp_total_asvs, tmp_get))
}
tmp_min_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- min(richness(get(i), index = "observed"))
   tmp_min_asvs <- c(append(tmp_min_asvs, tmp_get))
}
tmp_max_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- max(richness(get(i), index = "observed"))
   tmp_max_asvs <- c(append(tmp_max_asvs, tmp_get))
}
tmp_mean_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- round(mean(richness(get(i), index = "observed")$observed), digits = 0)
   tmp_mean_asvs <- c(append(tmp_mean_asvs, tmp_get))
}
tmp_med_asvs <- c()
for (i in tmp_samples) {
   tmp_get <- median(richness(get(i), index = "observed")$observed)
   tmp_med_asvs <- c(append(tmp_med_asvs, tmp_get))
}

tmp_bind <- data.frame(do.call("rbind", list(
  tmp_total_reads, tmp_min_read, tmp_max_read, tmp_mean_reads, tmp_median_reads, 
  tmp_total_asvs, tmp_min_asvs, tmp_max_asvs, tmp_mean_asvs, tmp_med_asvs)))

its_sum_pime <- dplyr::bind_cols(tmp_objects, tmp_bind) %>%
  dplyr::rename("Metric" = 1, "Full data" = 2, "PIME filter" = 3)
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_its("its_pime_filt_summary")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- its_sum_pime
seq_table %>%
  download_this(
    output_name = "its_pime_filt_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 150
    ), 
  columns = list(
    Metric = colDef(name = "Metric", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200)
    ), 
  searchable = FALSE, defaultPageSize = 10, showPagination = FALSE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = FALSE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

### Estimate Error in Prediction

Using  the function `pime.error.prediction` we can estimate the error in prediction. For each prevalence interval, this function randomizes the samples labels into arbitrary groupings using `n` random permutations, defined by the `bootstrap` value. For each, randomized and prevalence filtered, data set the OOB error rate is calculated to estimate whether the original differences in groups of samples occur by chance. Results are in a list containing a table and a box plot summarizing the results.

> This step takes a **really** long time to run. The code is included below in case you would like to perform this analysis.

<details markdown="1">
<summary>**Code** to Estimate Error in Prediction</summary>
```{r}
its_randomized <- pime.error.prediction(its_pime_ds, "TEMP",
                                          bootstrap = 100, parallel = TRUE,
                                          max.prev = 95)
```

```{r, echo=FALSE}
its_oob_error <- its_randomized$`Results table`
```

<small>`r caption_tab_its("its_pime_est_error")`</small>

```{r, echo=FALSE, eval=FALSE}
seq_table <- its_oob_error
seq_table %>%
  download_this(
    output_name = "its_pime_est_error",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body-outset", eval=FALSE}
seq_table <- its_oob_error  %>% 
 mutate_if(is.numeric, round, digits = 4)

reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 4),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 120
    ), 
  searchable = FALSE, defaultPageSize = 5, showPagination = TRUE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

```{r, echo=FALSE}
its_randomized$Plot
```
</details>

It is also possible to estimate the variation of OOB error for each prevalence interval filtering. This is done by running the random forests classification for `n` times, determined by the `bootstrap` value. The function will return a box plot figure and a table for each classification error.

```{r, code_folding=TRUE}
its_replicated.oob.error <- pime.oob.replicate(its_prevalences, "TEMP",
                                         bootstrap = 100, parallel = TRUE)
```

```{r, echo=FALSE, eval=TRUE, fig.height=3}
its_obb_orig <- its_replicated.oob.error$Plot
its_obb_orig <- its_obb_orig +
  theme() +
  labs(title = "OOB error Original data set")
its_obb_orig
```
<small>`r caption_fig_its("its_pime_obb_plot")`</small>

```{r, echo=FALSE, eval=FALSE, fig.height=5}
## set eval=TRUE TO include random plot
## and remove previous chunk

its_obb_orig <- its_replicated.oob.error$Plot
its_obb_rand <- its_randomized$Plot

its_obb_orig <- its_obb_orig +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank()) +
  labs(title = "OOB error Original data set")

its_obb_rand <- its_obb_rand  +
  labs(title = "OOB error Randomized data set")
its_obb_orig / its_obb_rand
```

To obtain the confusion matrix from random forests classification use the following:

```{r, code_folding=TRUE}
its_prev_confuse <- paste("its_best.prev$`Confusion`$", its_best, sep = "")
eval(parse(text = (its_prev_confuse)))
```

### Save Phyloseq PIME objects

Here we save a PIME filtered phyloseq object and add a tree.

```{r, code_folding=TRUE}
its_ps_pime <- its_prevalence_best
its_ps_pime_tree <- rtree(ntaxa(its_ps_pime), rooted = TRUE,
                           tip.label = taxa_names(its_ps_pime))
its_ps_pime <- merge_phyloseq(its_ps_pime,
                               sample_data,
                               its_ps_pime_tree)
saveRDS(its_ps_pime, "files/filtering/pime/rdata/its_ps_pime.rds")
```

### Split & save by predictor variable

```{r, code_folding=TRUE}
its_ps_pime_split <- pime.split.by.variable(its_ps_pime, "YR_TREAT")
saveRDS(its_ps_pime_split$Y0_CTL, "files/filtering/pime/rdata/its_ps_pime_Y0_CTL.rds")
saveRDS(its_ps_pime_split$Y0_N, "files/filtering/pime/rdata/its_ps_pime_Y0_N.rds")
saveRDS(its_ps_pime_split$Y0_NP, "files/filtering/pime/rdata/its_ps_pime_Y0_NP.rds")
saveRDS(its_ps_pime_split$Y0_P, "files/filtering/pime/rdata/its_ps_pime_Y0_P.rds")
saveRDS(its_ps_pime_split$Y1_CTL, "files/filtering/pime/rdata/its_ps_pime_Y1_CTL.rds")
saveRDS(its_ps_pime_split$Y1_N, "files/filtering/pime/rdata/its_ps_pime_Y1_N.rds")
saveRDS(its_ps_pime_split$Y1_NP, "files/filtering/pime/rdata/its_ps_pime_Y1_NP.rds")
saveRDS(its_ps_pime_split$Y1_P, "files/filtering/pime/rdata/its_ps_pime_Y1_P.rds")
saveRDS(its_ps_pime_split$Y4_CTL, "files/filtering/pime/rdata/its_ps_pime_Y4_CTL.rds")
saveRDS(its_ps_pime_split$Y4_N, "files/filtering/pime/rdata/its_ps_pime_Y4_N.rds")
saveRDS(its_ps_pime_split$Y4_NP, "files/filtering/pime/rdata/its_ps_pime_Y4_NP.rds")
saveRDS(its_ps_pime_split$Y4_P, "files/filtering/pime/rdata/its_ps_pime_Y4_P.rds")
```

<details markdown="1">
<summary>**Phyloseq objects** for each split</summary>
```{r, echo=FALSE, eval=TRUE}
its_ps_pime_split
```
</details>

```{r, echo=FALSE}
its_pime_preval.tax <- tax_table(its_ps_pime)
write.table(its_pime_preval.tax,
            file="files/filtering/pime/tables/its_asv_PIME_tax_table.txt",
            sep = "\t", quote = FALSE)

its_pime_preval.asv <- otu_table(t(its_ps_pime))
write.table(its_pime_preval.asv,
            file="files/filtering/pime/tables/its_asv_PIME_otu_table.txt",
            sep = "\t", quote = FALSE)

its_pime_preval.samp <- sample_data(its_ps_pime)
write.table(its_pime_preval.samp,
            file="files/filtering/pime/tables/its_asv_PIME_sample_data.txt",
            sep = "\t", quote = FALSE)
```

### Summary

How did filtering affect total reads and ASVs for each sample?

```{r, echo=FALSE}
tmp_read_count <- data.frame(sample_sums(its_ps_pime)) %>%
  tibble::rownames_to_column("SamName")
tmp_asv_count <- estimate_richness(its_ps_pime, split = TRUE, measures = "Observed") %>%
  tibble::rownames_to_column("SamName")
tmp_samp_data <- data.frame(sample_data(its_ps_pime))
tmp_samp_data <- tmp_samp_data[order(tmp_samp_data$YEAR, tmp_samp_data$TREAT),]

tmp_join <- dplyr::left_join(tmp_samp_data, tmp_read_count, by = "SamName") %>%
  dplyr::left_join(., tmp_asv_count, by = "SamName") %>%
  dplyr::rename("Sample_ID" = 1) %>%
  dplyr::rename("total_reads" = 9) %>%
  dplyr::rename("total_asvs" = 10)
its_pime_samp_summary <- tmp_join
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_its("its_pime_samp_summary")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- its_pime_samp_summary
seq_table %>%
  download_this(
    output_name = "its_pime_samp_summary",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-page", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = TRUE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    Sample_ID = colDef(name = "Metric", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 200),
     total_reads = colDef(name = "total reads", filterable = FALSE, minWidth = 140),
     total_asvs = colDef(name = "total asvs", filterable = FALSE, minWidth = 140)
    ), 
  searchable = TRUE, defaultPageSize = 5, showPagination = TRUE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```

And here is how the subsets changed through the PIME filtering process.

```{r, echo=FALSE, eval=TRUE}
its_ps_pime_Y0_CTL <- its_ps_pime_split$`Y0_CTL`
its_ps_pime_Y0_N <- its_ps_pime_split$`Y0_N`
its_ps_pime_Y0_NP <- its_ps_pime_split$`Y0_NP`
its_ps_pime_Y0_P <- its_ps_pime_split$`Y0_P`
its_ps_pime_Y1_CTL <- its_ps_pime_split$`Y1_CTL`
its_ps_pime_Y1_N <- its_ps_pime_split$`Y1_N`
its_ps_pime_Y1_NP <- its_ps_pime_split$`Y1_NP`
its_ps_pime_Y1_P <- its_ps_pime_split$`Y1_P`
its_ps_pime_Y4_CTL <- its_ps_pime_split$`Y4_CTL`
its_ps_pime_Y4_N <- its_ps_pime_split$`Y4_N`
its_ps_pime_Y4_NP <- its_ps_pime_split$`Y4_NP`
its_ps_pime_Y4_P <- its_ps_pime_split$`Y4_P`

tmp_objects <- data.frame(c("FULL data set", "Rarefied data", "PIME filtered data",
                            "PIME (Y0_CTL)", "PIME (Y0_N)", "PIME (Y0_NP)", 
                            "PIME (Y0_P)", "PIME (Y1_CTL)", "PIME (Y1_N)", 
                            "PIME (Y1_NP)", "PIME (Y1_P)", "PIME (Y4_CTL)", 
                            "PIME (Y4_N)", "PIME (Y4_NP)", "PIME (Y4_P)"))
                            
                            
tmp_samples <- c("its_ps_work", "its_pime_ds", "its_ps_pime",
                 "its_ps_pime_Y0_CTL", "its_ps_pime_Y0_N", "its_ps_pime_Y0_NP", 
                 "its_ps_pime_Y0_P", "its_ps_pime_Y1_CTL", "its_ps_pime_Y1_N", 
                 "its_ps_pime_Y1_NP", "its_ps_pime_Y1_P", "its_ps_pime_Y4_CTL", 
                 "its_ps_pime_Y4_N", "its_ps_pime_Y4_NP", "its_ps_pime_Y4_P")
tmp_no_samp <- c()
for (i in tmp_samples) {
   tmp_get <- nsamples(get(i))
   tmp_no_samp <- c(append(tmp_no_samp, tmp_get))
}
tmp_no_samp <- data.frame(tmp_no_samp)

tmp_rc <- c()
for (i in tmp_samples) {
   tmp_get <- sum(readcount(get(i)))
   tmp_rc <- c(append(tmp_rc, tmp_get))
}
tmp_rc <- data.frame(tmp_rc)
tmp_asv <- c()
for (i in tmp_samples) {
   tmp_get <- ntaxa(get(i))
   tmp_asv <- c(append(tmp_asv, tmp_get))
}
tmp_asv <- data.frame(tmp_asv)

its_asv_pime_sum <- dplyr::bind_cols(tmp_objects, tmp_no_samp) %>%
                      dplyr::bind_cols(., tmp_rc) %>%
                      dplyr::bind_cols(., tmp_asv) %>%
  dplyr::rename("Description" = 1, "no. samples" = 2,
                "total reads" = 3, "total asvs" = 4)
rm(list = ls(pattern = "tmp_"))
```

<small>`r caption_tab_its("its_asv_pime_sum")`</small>

```{r, echo=FALSE, eval=TRUE}
seq_table <- its_asv_pime_sum
seq_table %>%
  download_this(
    output_name = "its_asv_pime_sum",
    output_extension = ".csv",
    button_label = "Download data as csv file",
    button_type = "default",
    csv2 = FALSE,
    has_icon = TRUE,
    icon = "fa fa-save")
```

```{r, echo=FALSE, layout="l-body", eval=TRUE}
reactable(seq_table,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = FALSE, sortable = FALSE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold"), minWidth = 100
    ), 
  columns = list(
    Description = colDef(name = "Description", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 150)
    ), 
  searchable = FALSE, defaultPageSize = 15, showPagination = FALSE,
  pageSizeOptions = c(5, 10, nrow(seq_table)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = TRUE, 
  wrap = FALSE, showSortable = FALSE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em")))

rm(seq_table)
```


```{r, echo=FALSE}
save.image("page_build/pime_its_asv_wf.rdata")
objects()
```

And that is all for filtering. The last thing we will do is add the new variable we created for the PIME filtered data sets to the other phyloseq objects. We will use  these objects for all downstream  analyses.

```{r , code_folding=TRUE}
remove(list = ls())
ssu_ps_work <- readRDS("files/data-prep/rdata/ssu_ps_work.rds")
ssu_ps_filt <- readRDS("files/filtering/arbitrary/rdata/ssu_ps_filt.rds")
ssu_ps_perfect <- readRDS("files/filtering/perfect/rdata/ssu_ps_perfect.rds")
its_ps_work <- readRDS("files/data-prep/rdata/its_ps_work.rds")
its_ps_filt <- readRDS("files/filtering/arbitrary/rdata/its_ps_filt.rds")
its_ps_perfect <- readRDS("files/filtering/perfect/rdata/its_ps_perfect.rds")

ps_list <-  c("ssu_ps_work", "ssu_ps_filt", "ssu_ps_perfect", 
              "its_ps_work", "its_ps_filt", "its_ps_perfect")

for (i in ps_list){
  tmp_path <- "files/filtering/working_ps/"
  tmp_get <- get(i)
  tmp_samp_data <- sample_data(tmp_get)	
  tmp_samp_data$YR_TREAT <- paste(tmp_samp_data$YEAR, tmp_samp_data$TREAT, sep="_")
  tmp_ps <- merge_phyloseq(tmp_get, tmp_samp_data)	
  assign(i, tmp_ps)
  saveRDS(tmp_ps, paste(tmp_path, i, ".rds", sep = ""))
  rm(list = ls(pattern = "tmp_"))
}
```


```{r echo=FALSE, eval=TRUE}
remove(list = ls())
```

##  Source Code {.appendix}

The source code for this page can be accessed on GitHub by [clicking this link](https://github.com/agua-salud/nutrients/blob/main/filtering.Rmd).

## Data Availability {.appendix}

Data generated in this workflow and the Rdata need to run the workflow can be accessed on figshare at [10.25573/data.14701440](https://doi.org/10.25573/data.14701440).